{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import argparse\n",
    "import os\n",
    "import cv2 \n",
    "import random\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/Users/liaoqt/documents/CS294/data\"\n",
    "CATEGORIES = [\"Boring\", \"Interesting\"]\n",
    "training_data = []\n",
    "feature_extraction_data = []\n",
    "IMG_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stitch the images together \n",
    "## 0 1 2 \n",
    "## 3 4 5\n",
    "## 6 7 8\n",
    "\n",
    "def stitch_images(file_path, file_name):\n",
    "    images = [Image.open(image) for image in [file_path + \"/\" + file_name + str(x) + \".png\" for x in range(100, 109)]]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = int(sum(widths) / 3)\n",
    "    total_height = int(sum(heights) / 3)\n",
    "    new_image = Image.new(\"RGB\", (total_width, total_height))\n",
    "    for index in range(0, 9):\n",
    "        image = images[index]\n",
    "        new_image.paste(image, ((index % 3) * image.size[0], math.floor(index / 3) * image.size[1]))\n",
    "\n",
    "    IMAGE_DIR = os.path.join(file_path, \"combined/\") + file_name + \"combined.png\"\n",
    "    new_image.save(IMAGE_DIR)\n",
    "    return IMAGE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for category in CATEGORIES: \n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for image in sorted(os.listdir(path)): \n",
    "            if \"100\" not in image: ## Find the starting frame\n",
    "                continue\n",
    "            IMAGE_DIR = stitch_images(path, image[0:-7])\n",
    "            try: \n",
    "                img_array = cv2.imread(IMAGE_DIR, cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "                if class_num == 0:\n",
    "                    feature_extraction_data.append([IMAGE_DIR, False])\n",
    "                else: \n",
    "                    feature_extraction_data.append([IMAGE_DIR, True])\n",
    "                \n",
    "            except Exception as e: \n",
    "                pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(feature_extraction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, labels = [], []\n",
    "for image_path, label in feature_extraction_data: \n",
    "    image_paths.append(image_path)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "def compute_entropy(signal):\n",
    "    lensig = signal.size\n",
    "    symset = list(set(signal))\n",
    "    numsym = len(symset)\n",
    "    propab = [np.size(signal[signal == i]) / (1.0 * lensig) for i in symset]\n",
    "    entropy = np.sum([p * np.log2(1.0 / p) for p in propab])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropys = []\n",
    "# for i in range(len(image_paths)): \n",
    "image = np.array(Image.open(image_paths[0]).convert(\"L\"))\n",
    "N = 5\n",
    "S = image.shape\n",
    "entropy = 0.0\n",
    "for row in range(S[0]):\n",
    "    for col in range(S[1]):\n",
    "        Lx = max(0, col - N)\n",
    "        Ux = min(S[1], col + N)\n",
    "        Ly = max(0, row - N)\n",
    "        Uy = min(S[0], row + N)\n",
    "        region = image[Ly: Uy, Lx : Ux].flatten()\n",
    "        entropy += compute_entropy(region)\n",
    "entropys.append([entropy, labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretrainedmodels\n",
    "import pretrainedmodels.utils as utils\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "model_name = 'nasnetalarge'\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "model.eval()\n",
    "\n",
    "load_img = utils.LoadImage()\n",
    "tf_img = utils.TransformImage(model)\n",
    "features_file = open(\"file.csv\", \"ab\")\n",
    "feature_data = []\n",
    "\n",
    "\n",
    "for i in range(len(image_paths)):\n",
    "        input_img = load_img(image_paths[i])\n",
    "        input_tensor = tf_img(input_img)\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        input = torch.autograd.Variable(input_tensor, requires_grad=False)\n",
    "        output_logits = model(input)\n",
    "        output_features = model.features(input)\n",
    "        output_logits = model.logits(output_features)\n",
    "        output_logits = output_logits[0].detach().numpy()\n",
    "        row_data = np.append(output_logits, labels[i])\n",
    "        feature_data = np.append(feature_data, row_data)\n",
    "        \n",
    "#         np.savetxt(\"file.csv\", output_logits[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = feature_data.reshape(44, 1001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"file.csv\", feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_features import image_features\n",
    "n = len(image_paths)\n",
    "X_train = image_features(image_paths[:n], progress=True)\n",
    "y_train = no_layer[:n]\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for features, label in training_data: \n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data \n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load saved Data\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\", \"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Training \n",
    "### Data Normalization\n",
    "X = X / 255.0 \n",
    "model = keras.Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# batch_size should be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=16, epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
