{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import argparse\n",
    "import os\n",
    "import cv2 \n",
    "import random\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/Users/liaoqt/documents/github/mlca/finaldata\"\n",
    "CATEGORIES = [\"Boring\", \"Interesting\"]\n",
    "training_data = []\n",
    "feature_extraction_data = []\n",
    "IMG_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stitch the images together \n",
    "## 0 1 2 \n",
    "## 3 4 5\n",
    "## 6 7 8\n",
    "\n",
    "def stitch_images(file_path, file_name):\n",
    "    images = [Image.open(image) for image in [file_path + \"/\" + file_name + str(x) + \".png\" for x in range(100, 109)]]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = int(sum(widths) / 3)\n",
    "    total_height = int(sum(heights) / 3)\n",
    "    new_image = Image.new(\"RGB\", (total_width, total_height))\n",
    "    for index in range(0, 9):\n",
    "        image = images[index]\n",
    "        new_image.paste(image, ((index % 3) * image.size[0], math.floor(index / 3) * image.size[1]))\n",
    "    IMAGE_DIR = os.path.join(file_path, \"combined/\") + file_name + \"combined.png\"\n",
    "    new_image.save(IMAGE_DIR)\n",
    "    return IMAGE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for category in CATEGORIES: \n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for image in sorted(os.listdir(path)): \n",
    "            if \"100\" not in image: ## Find the starting frame\n",
    "                continue\n",
    "            IMAGE_DIR = stitch_images(path, image[0:-7])\n",
    "            try: \n",
    "                img_array = cv2.imread(IMAGE_DIR, cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "                if class_num == 0:\n",
    "                    feature_extraction_data.append([IMAGE_DIR, False])\n",
    "                else: \n",
    "                    feature_extraction_data.append([IMAGE_DIR, True])\n",
    "                \n",
    "            except Exception as e: \n",
    "                pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(feature_extraction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, labels = [], []\n",
    "for image_path, label in feature_extraction_data: \n",
    "    image_paths.append(image_path)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "def compute_entropy(signal):\n",
    "    lensig = signal.size\n",
    "    symset = list(set(signal))\n",
    "    numsym = len(symset)\n",
    "    propab = [np.size(signal[signal == i]) / (1.0 * lensig) for i in symset]\n",
    "    entropy = np.sum([p * np.log2(1.0 / p) for p in propab])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropys = []\n",
    "# for i in range(len(image_paths)): \n",
    "image = np.array(Image.open(image_paths[0]).convert(\"L\"))\n",
    "N = 5\n",
    "S = image.shape\n",
    "entropy = 0.0\n",
    "for row in range(S[0]):\n",
    "    for col in range(S[1]):\n",
    "        Lx = max(0, col - N)\n",
    "        Ux = min(S[1], col + N)\n",
    "        Ly = max(0, row - N)\n",
    "        Uy = min(S[0], row + N)\n",
    "        region = image[Ly: Uy, Lx : Ux].flatten()\n",
    "        entropy += compute_entropy(region)\n",
    "entropys.append([entropy, labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretrainedmodels\n",
    "import pretrainedmodels.utils as utils\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "model_name = 'nasnetalarge'\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "model.eval()\n",
    "\n",
    "load_img = utils.LoadImage()\n",
    "tf_img = utils.TransformImage(model)\n",
    "features_file = open(\"file.csv\", \"ab\")\n",
    "feature_data = []\n",
    "\n",
    "for i in range(len(image_paths)):\n",
    "        input_img = load_img(image_paths[i])\n",
    "        input_tensor = tf_img(input_img)\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        input = torch.autograd.Variable(input_tensor, requires_grad=False)\n",
    "        output_logits = model(input)\n",
    "        output_features = model.features(input)\n",
    "        output_logits = model.logits(output_features)\n",
    "        output_logits = output_logits[0].detach().numpy()\n",
    "        row_data = np.append(output_logits, labels[i])\n",
    "        feature_data = np.append(feature_data, row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = feature_data.reshape(44, 1001)\n",
    "np.savetxt(\"file.csv\", feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_features import image_features\n",
    "n = len(image_paths)\n",
    "X_train = image_features(image_paths[:n], progress=True)\n",
    "y_train = no_layer[:n]\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for features, label in training_data: \n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "540/540 [==============================] - 264s 489ms/step - loss: 4.7680 - accuracy: 0.5574\n",
      "Epoch 2/20\n",
      "540/540 [==============================] - 261s 484ms/step - loss: 2.7508 - accuracy: 0.7019\n",
      "Epoch 3/20\n",
      "540/540 [==============================] - 266s 492ms/step - loss: 2.9188 - accuracy: 0.6935\n",
      "Epoch 4/20\n",
      "540/540 [==============================] - 270s 500ms/step - loss: 1.3066 - accuracy: 0.6926\n",
      "Epoch 5/20\n",
      "540/540 [==============================] - 270s 499ms/step - loss: 1.4138 - accuracy: 0.6935\n",
      "Epoch 6/20\n",
      "540/540 [==============================] - 262s 485ms/step - loss: 1.4089 - accuracy: 0.6935\n",
      "Epoch 7/20\n",
      "540/540 [==============================] - 262s 485ms/step - loss: 1.1154 - accuracy: 0.6991\n",
      "Epoch 8/20\n",
      "540/540 [==============================] - 261s 483ms/step - loss: 1.7903 - accuracy: 0.7102\n",
      "Epoch 9/20\n",
      "540/540 [==============================] - 257s 476ms/step - loss: 1.6458 - accuracy: 0.7046\n",
      "Epoch 10/20\n",
      "540/540 [==============================] - 256s 474ms/step - loss: 1.2641 - accuracy: 0.6750\n",
      "Epoch 11/20\n",
      "540/540 [==============================] - 258s 478ms/step - loss: 0.7946 - accuracy: 0.7139\n",
      "Epoch 12/20\n",
      "540/540 [==============================] - 253s 469ms/step - loss: 0.5718 - accuracy: 0.6972\n",
      "Epoch 13/20\n",
      "540/540 [==============================] - 253s 468ms/step - loss: 1.1506 - accuracy: 0.7491\n",
      "Epoch 14/20\n",
      "540/540 [==============================] - 246s 456ms/step - loss: 0.7303 - accuracy: 0.7593\n",
      "Epoch 15/20\n",
      "540/540 [==============================] - 247s 457ms/step - loss: 1.2374 - accuracy: 0.7287\n",
      "Epoch 16/20\n",
      "540/540 [==============================] - 248s 459ms/step - loss: 1.0195 - accuracy: 0.7454\n",
      "Epoch 17/20\n",
      "540/540 [==============================] - 254s 470ms/step - loss: 0.6527 - accuracy: 0.7361\n",
      "Epoch 18/20\n",
      "540/540 [==============================] - 268s 495ms/step - loss: 1.4058 - accuracy: 0.7500\n",
      "Epoch 19/20\n",
      "540/540 [==============================] - 276s 512ms/step - loss: 1.4078 - accuracy: 0.7713\n",
      "Epoch 20/20\n",
      "540/540 [==============================] - 279s 516ms/step - loss: 1.0311 - accuracy: 0.7602\n",
      "Accuracy on test with 1200 ---\n",
      "0.925\n",
      "1.0\n",
      "Epoch 1/20\n",
      "675/675 [==============================] - 345s 512ms/step - loss: 7.3447 - accuracy: 0.5326\n",
      "Epoch 2/20\n",
      "239/675 [=========>....................] - ETA: 3:34 - loss: 3.1715 - accuracy: 0.5209"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "X = X.astype('float32')\n",
    "X = X / 255.0\n",
    "# Split X, y into training and test data\n",
    "\n",
    "data_sizes = [1200, 1500, 1800, 2100]\n",
    "for size in data_sizes: \n",
    "    tempx = X[:size]\n",
    "    tempy = y[:size]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tempx, tempy, test_size = 0.1, random_state = 42)\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=tempx.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.15))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "      model.fit(X_train, y_train, batch_size=2, epochs=20, verbose = 1) \n",
    "\n",
    "    # Do an accuracy test on data with only ~10% interesting claseses\n",
    "    print('Accuracy on test with {data_size} ---'.format(data_size = size))\n",
    "    y_pred_test=model.predict_classes(X_test)\n",
    "    print(accuracy_score(y_test,y_pred_test))\n",
    "    print(recall_score(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 300, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=16, epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
